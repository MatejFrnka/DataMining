{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 1\n",
    "Download data at https://github.com/MatejFrnka/DataMining/tree/master/HW1\n",
    "folder data contains 200 news articles\n",
    "folder data 2 contains 930 news articles\n",
    "\n",
    "## Runtimes:\n",
    "For the smaller dataset (folder data), minhash function (see at the bottom of the document) performed the best:\n",
    "* Naive: ~0.71s\n",
    "* Minhash: ~0.58s\n",
    "* LSH: ~0.64\n",
    "\n",
    "For the larger dataset (folder data 2), the lower asymptotic complexity of LSH makes LSH perform the fastest:\n",
    "* Naive: ~15s\n",
    "* Minhash: ~4.5s\n",
    "* LSH: ~3.5s\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from data import load_data\n",
    "import itertools\n",
    "\n",
    "MAX_INT = 2 ** 31 - 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shingling\n",
    "Separate string into k long shingles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Shingling:\n",
    "\n",
    "    def __init__(self, k):\n",
    "        \"\"\"\n",
    "        :param k: Size of shingle\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def get_shingles(self, string):\n",
    "        \"\"\"\n",
    "        :param string: String to turn into shingles\n",
    "        :return: Set of hashed shingles\n",
    "        \"\"\"\n",
    "        shingles = set()\n",
    "        for i in range(len(string) - self.k + 1):\n",
    "            # Hash string to save space and make it easier to work with\n",
    "            shingles.add(hash(string[i:i + self.k]))\n",
    "        return shingles\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute Jaccard similarity\n",
    "Simply follow the formula of jaccard similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class CompareSets:\n",
    "    def jaccard_similarity(self, a, b) -> float:\n",
    "        \"\"\"\n",
    "        :param a: Set of elements\n",
    "        :param b: Set of elements\n",
    "        :return: Number between 0 and 1 representing the similarity in percentage. 1=100% similar sets\n",
    "        \"\"\"\n",
    "        return len(a.intersection(b)) / len(a.union(b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Min hash algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "class MinHashing:\n",
    "\n",
    "    def __init__(self, hasher_cnt):\n",
    "        \"\"\"\n",
    "        :param hasher_cnt: Amount of hashing functions, the higher the number is, the higher the compute time and accuracy\n",
    "        \"\"\"\n",
    "        self.hasher_cnt = hasher_cnt\n",
    "        # hashing function is x*multiplier + bias % mod\n",
    "        # get random multipliers for each of the hashing functions\n",
    "        self.multiplier = np.random.randint(1, MAX_INT, size=hasher_cnt)\n",
    "        # get random bias for each of the hashing functions\n",
    "        self.bias = np.random.randint(0, MAX_INT, size=hasher_cnt)\n",
    "        # get mod, higher mod = fewer collisions\n",
    "        self.mod = MAX_INT\n",
    "\n",
    "    def get_document_matrix(self, shingle_sets):\n",
    "        \"\"\"\n",
    "        :param shingle_sets: Array of sets of shingles, every element in array represents a document\n",
    "        :return: Matrix with columns representing each document and with rows for every unique element in the input shingle_sets. Matrix contains True if document contains the row's element and False if it doesn't\n",
    "        \"\"\"\n",
    "        all_elements_array = list(set().union(*shingle_sets))\n",
    "        document_matrix = []\n",
    "        for i in range(len(shingle_sets)):\n",
    "            # run for every document\n",
    "            # create array for every element in all_elements containing True if document contains shingle and False if not\n",
    "            document_matrix.append(\n",
    "                np.array([k in shingle_sets[i] for k in all_elements_array]))\n",
    "        return np.array(document_matrix).T\n",
    "\n",
    "    def apply_hashers(self, val):\n",
    "        \"\"\"\n",
    "        :param val: Value to apply hashers to\n",
    "        :return: array of outputs of the hashes\n",
    "        \"\"\"\n",
    "        return np.array(\n",
    "            (np.repeat(val, self.hasher_cnt) * self.multiplier + self.bias) %\n",
    "            self.mod)\n",
    "\n",
    "    def get_signature(self, document_matrix):\n",
    "        \"\"\"\n",
    "        :param document_matrix: Document matrix\n",
    "        :return: Signature of the document matrix for each hashing function\n",
    "        \"\"\"\n",
    "        # initiate signature with maximum values\n",
    "        res_signature = np.ones((document_matrix.shape[1], self.hasher_cnt)) * MAX_INT\n",
    "        for i, row in enumerate(document_matrix):\n",
    "            # simulate permuting by using different hash functions\n",
    "            hashed_i = self.apply_hashers(i)\n",
    "            for k, col in enumerate(row):\n",
    "                # if column of the document_matrix contains True, we can update the signature\n",
    "                if col:\n",
    "                    # update signature if new value is smaller than the existing\n",
    "                    res_signature[k] = np.minimum(res_signature[k], hashed_i)\n",
    "        return res_signature\n",
    "\n",
    "    def minhash(self, shingles_sets):\n",
    "        \"\"\"\n",
    "        :param shingles_sets: Array of sets of shingles, every element in array represents a document\n",
    "        :return: Matrix document count x hashers count containing signature for every hashing function\n",
    "        \"\"\"\n",
    "        document_matrix = self.get_document_matrix(shingles_sets)\n",
    "        return self.get_signature(document_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compare signatures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class CompareSignatures:\n",
    "    def compare(self, a, b):\n",
    "        \"\"\"\n",
    "        :param a: Array of signatures of given document\n",
    "        :param b: Array of signatures of given document\n",
    "        :return: Percentage of matching items\n",
    "        \"\"\"\n",
    "        return np.sum(a == b) / len(a)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSH"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class LSH:\n",
    "\n",
    "    def sep_siganture(self, sig, bands):\n",
    "        \"\"\"\n",
    "        :param sig: Matrix of documents and their signatures\n",
    "        :param bands: Amount of bands to split array into\n",
    "        :return: Array of smaller matrixes of size document count x signature count / bands\n",
    "        \"\"\"\n",
    "        return np.array_split(sig, bands, axis=1)\n",
    "\n",
    "    def get_candidate_pairs(self, signature, band_cnt, bucket_cnt=10_000):\n",
    "        \"\"\"\n",
    "        :param signature: Matrix of signatures (documents x signature)\n",
    "        :param band_cnt: Number of bands\n",
    "        :param bucket_cnt: Number of buckets, higher number decreases collisions but increases runtime\n",
    "        :return: Dictionary with pairs as keys and count of their similar bands\n",
    "        \"\"\"\n",
    "        bands = self.sep_siganture(signature, band_cnt)\n",
    "        # Generate buckets to put distribute documents to\n",
    "        buckets = [[[] for _ in range(bucket_cnt)] for _ in range(band_cnt)]\n",
    "\n",
    "        for i, band in enumerate(bands):\n",
    "            for k, doc in enumerate(band):\n",
    "                # distribute to buckets based on band content\n",
    "                buckets[i][hash(str(doc)) % bucket_cnt].append(k)\n",
    "\n",
    "        # get candidate pairs\n",
    "        candidate_pairs = dict()\n",
    "        # iterate all buckets and keep those, with at least two elements\n",
    "        for band in buckets:\n",
    "            for slot in band:\n",
    "                if len(slot) > 1:\n",
    "                    # more than 1 document in a bucket, documents might be similar\n",
    "                    for c in itertools.combinations(slot, 2):\n",
    "                        c_frozen = frozenset(c)\n",
    "                        if c_frozen in candidate_pairs:\n",
    "                            candidate_pairs[c_frozen] += 1\n",
    "                        else:\n",
    "                            candidate_pairs[c_frozen] = 1\n",
    "\n",
    "        return candidate_pairs\n",
    "\n",
    "    def lsh(self, signature, band_cnt, similarity, bucket_cnt=1_000):\n",
    "        \"\"\"\n",
    "        :param signature: Matrix of signatures (documents x signature)\n",
    "        :param band_cnt: Number of bands\n",
    "        :param similarity: Similarity threshold\n",
    "        :param bucket_cnt: Number of buckets, higher number decreases collisions but increases runtime\n",
    "        :return: Pairs of documents with similarity above the threshold\n",
    "        \"\"\"\n",
    "        # get candidate pairs\n",
    "        candidate_pairs = self.get_candidate_pairs(signature, band_cnt, bucket_cnt)\n",
    "        # similarity threshold\n",
    "        row_cnt = signature.shape[1] // band_cnt\n",
    "        print(f\"LSH tuned for {round((1 / band_cnt) ** (1 / row_cnt) * 100, 1)}% similarity\")\n",
    "        res = []\n",
    "        comparator = CompareSignatures()\n",
    "        # Compare only similar pairs\n",
    "        for pair in candidate_pairs:\n",
    "            p = list(pair)\n",
    "            if comparator.compare(signature[p[0]], signature[p[1]]) > similarity:\n",
    "                res.append(p)\n",
    "        return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run different ways of finding similarity\n",
    "naive_similarity: uses jaccard similarity to compare sets of shingles\n",
    "minhash_similarity: uses minhash to find signatures and then compares every signature of every document with every other document\n",
    "lsh: finds candidate pairs and then compares their signatures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# helper function\n",
    "def filter_results(data, similarities, threshold):\n",
    "    filtered = filter(lambda x: x[0] > threshold, similarities)\n",
    "    sorted_sim = sorted(filtered, key=lambda x: x[0])\n",
    "    return [(data[k[1][0]][0], data[k[1][1]][0]) for k in sorted_sim]\n",
    "\n",
    "\n",
    "def naive_similarity(threshold, shingle_len):\n",
    "    data = load_data(-1)\n",
    "    shingler = Shingling(shingle_len)\n",
    "    # get shingles\n",
    "    data_shingles = [shingler.get_shingles(doc[1]) for doc in data]\n",
    "    similarities = []\n",
    "    # compare every document with every other\n",
    "    for i, doc_i in enumerate(data_shingles):\n",
    "        for o, doc_o in enumerate(data_shingles):\n",
    "            if i > o:\n",
    "                # write down similarity\n",
    "                similarities.append((CompareSets().jaccard_similarity(doc_i, doc_o), (i, o)))\n",
    "    # filter above threshold\n",
    "    return filter_results(data, similarities, threshold)\n",
    "\n",
    "\n",
    "def minhash_similarity(threshold, shingle_len, hasher_cnt):\n",
    "    data = load_data(-1)\n",
    "    shingler = Shingling(shingle_len)\n",
    "    similarities = []\n",
    "    # get shingles\n",
    "    data_shingles = [shingler.get_shingles(doc[1]) for doc in data]\n",
    "    minhash = MinHashing(hasher_cnt)\n",
    "    # get signatures\n",
    "    signature = minhash.minhash(data_shingles)\n",
    "    # compare every signature with every other signature\n",
    "    for i, val_i in enumerate(signature):\n",
    "        for o, val_o in enumerate(signature):\n",
    "            if i > o:\n",
    "                similarities.append((CompareSignatures().compare(val_i, val_o), (i, o)))\n",
    "    return filter_results(data, similarities, threshold)\n",
    "\n",
    "\n",
    "def lsh(threshold, shingle_len, hasher_cnt):\n",
    "    data = load_data()\n",
    "    shingler = Shingling(shingle_len)\n",
    "    data_shingles = [shingler.get_shingles(doc[1]) for doc in data]\n",
    "    minhash = MinHashing(hasher_cnt)\n",
    "    signature = minhash.minhash(data_shingles)\n",
    "    # run LSH\n",
    "    res = LSH().lsh(signature, 10, threshold, 100)\n",
    "    # transform into tuples\n",
    "    return [(data[r[0]][0], data[r[1]][0]) for r in res]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Define similarity parameters\n",
    "sim = 0.80\n",
    "shing = 3\n",
    "hashers = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('reut2-47.txt', 'reut2-34.txt'), ('reut2-85.txt', 'reut2-82.txt'), ('reut2-146.txt', 'reut2-134.txt')]\n",
      "0.7182106971740723\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "print(naive_similarity(sim, shing))\n",
    "print(time() - t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('reut2-47.txt', 'reut2-34.txt'), ('reut2-85.txt', 'reut2-82.txt'), ('reut2-146.txt', 'reut2-134.txt')]\n",
      "0.5832540988922119\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "print(minhash_similarity(sim, shing, hashers))\n",
    "print(time() - t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH tuned for 79.4% similarity\n",
      "[('reut2-134.txt', 'reut2-146.txt'), ('reut2-85.txt', 'reut2-82.txt'), ('reut2-34.txt', 'reut2-47.txt')]\n",
      "0.6449651718139648\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "print(lsh(sim, shing, hashers))\n",
    "# for small amount of documents is slower than minhash but asymptotically, it is faster\n",
    "# use folder data 2 in https://github.com/MatejFrnka/DataMining/tree/master/HW1 for more documents\n",
    "print(time() - t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}